{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNIKEY_COMP5046_Ass1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*\n",
        "\n",
        "## Documentation (4 marks)\n",
        "\n",
        "In the section 1,2, and 3, you are required to describe and justify any decisions you made for the final implementation. You can find the tag (Justify your decision) or (Explain the performance) for the point that you should justify the purpose of applying the specific technique/model and explain the performance.\n",
        "For example, for section 1 (preprocess data), you need to describe which pre-processing techniques (removing numbers, converting to lowercase, removing stop words, stemming, etc.) were conducted and justify your decision (the purpose of choosing a specific pre-processing techniques, and benefit of using that technique or the integration of techniques for your AI) in your ipynb file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21"
      },
      "source": [
        "# 1 - Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr8o7UZxWf3Y"
      },
      "source": [
        "If you want to know how data has been saved in pickle file: see this [ipynb file](https://drive.google.com/file/d/1ZQUVBzgH7N2EbiyE3WTPx7JNe2eRTs36/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988f1fc1-484f-4401-aba4-9832b82e5f47"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.pkl')  \n",
        "\n",
        "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.pkl')  \n",
        "\n",
        "import pickle\n",
        "training_data = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
        "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
        "print(\"------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "Size of training dataset: 8000\n",
            "Size of testing dataset: 2000\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe"
      },
      "source": [
        "\n",
        "Preprocess data: You are asked to pre-process the training set by integrating several text pre-processing techniques [Lab5] (e.g. tokenisation, removing numbers, converting to lowercase, removing stop words, stemming, etc.). You should justify the reason why you apply the specific preprocessing techniques (Justify your decision)\n",
        "\n",
        "## [2 Marks]\n",
        "\n",
        "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *\n",
        "\n",
        "*** replace this markdown cell with justifcations to each preprocessing step ***\n",
        "\n",
        "Lab 05"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g"
      },
      "source": [
        "# 2 - Model Implementation\n",
        "\n",
        "In this section, you are to implement three components, including Word Embedding module, Lexicon Embedding module, and Bi-directional RNN Sequence Model. For training, you are free to choose hyperparameters [Lab2,Lab4,Lab5] (e.g. dimension of embeddings, learning rate, epochs, etc.).\n",
        "\n",
        "## [7 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr"
      },
      "source": [
        "## 2.1. Word Embeddings\n",
        "\n",
        "First, you are asked to build a word embedding model (for representing word vectors, such as word2vec-CBOW, word2vec-Skip gram, fastText, and Glove) for the input embedding of your sequence model [Lab2]. Note that we used one-hot vectors as inputs for the sequence model in the Lab3 and Lab4. You are required to complete the following sections in the format:\n",
        "\n",
        "Preprocess data for word embeddings: You are to use and preprocess NLTK Twitter dataset (the one provided in the Section 1) and/or any Dataset (e.g. TED talk, Google News) for word embeddings [Lab2]. This can be different from the preprocessing technique that you used in Section 1. You can use both training and testing dataset in order to train the word embedding. (Justify your decision)\n",
        "\n",
        "Build training model for word embeddings: You are to build a training model for word embeddings. You are required to articulate the hyperparameters [Lab2] you chose (dimension of embeddings, window size, learning rate, etc.). Note that any word embeddings model [Lab2] (e.g. word2vec-CBOW, word2vec-Skip gram, fasttext, glove) can be applied. (Justify your decision)\n",
        "Train model: You are to train the model.\n",
        "\n",
        "*** use GENSIM package in Lab02 ***\n",
        "\n",
        "## [2 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-"
      },
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*\n",
        "\n",
        "*Includes justification on why you chose the word dataset to train on*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*\n",
        "\n",
        "articulate the hyperamater used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Lexicon Embeddings\n",
        "\n",
        "Then, you are to check whether each word is in the positive or negative lexicon. In this assignment, we will use the Opinion Lexicon (If you cannot downalod this, please right click and open in a new page or You can directly download from the data folder in this github), which includes a list of english positive and negative opinion words or sentiment words. (2006 positive and 4783 negative words)\n",
        "\n",
        "Each word needs to be converted into one-dimensional categorical embedding with three categories, such as not_exist(0), negative(1), and positive(2). This 0,1,2 categories will be used for the input for the Section 2.3 Bi-directional RNN Sequence model. *** i.e. check if the words in pickle dataset exist inside the Opinion Lexiscon Dataset ***\n",
        "\n",
        "NOTE: If you want to use more than one-dimensional or not using categorical embedding, please (Justify your decision)\n",
        "\n",
        "** open in colab to download opinion dataset **\n",
        "## [2 marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Lexicon-based Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKbLnN-3GlI1"
      },
      "source": [
        "*[Optional] You are required to describe why you would like to use more than one-dimensional embedding.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2CUCL1cGlI2"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd"
      },
      "source": [
        "## 2.3. Bi-directional RNN Sequence model\n",
        "\n",
        "Finally, you are asked to build the Many-to-One (N to 1) Sequence model in order to detect the sentiment/emotion. \n",
        "\n",
        "*** Note that your model should be the best model selected from the evaluation *** (will be discussed in the Section 3. Evaluation). You are required to implement the following functions:\n",
        "\n",
        "Apply/Import Word and Lexicon Embedding as an input: You are to concatenate the trained word embedding and lexicon embedding, and apply to the sequence model\n",
        "\n",
        "Build training sequence model: You are to build the Bi-directional RNN-based (Bi-RNN or Bi-LSTM or Bi-GRU) Many-to-One (N to One) sequence model (N: word, One: Sentiment - Positive or Negative). You are required to describe how hyperparameters [Lab4,Lab5] (the Number of Epochs, learning rate, etc.) were decided. (Justify your decision)\n",
        "\n",
        "Train model: While the model is being trained, you are required to display the Training Loss and the Number of Epochs. [Lab4,Lab5]\n",
        "Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.\n",
        "\n",
        "## [3 Marks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Lexicon Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R"
      },
      "source": [
        "### 2.3.3. Train Sequence Model\n",
        "\n",
        "Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN"
      },
      "source": [
        "# 3 - Evaluation\n",
        "\n",
        "**(Word Embedding Evaluation 3 marks)**: Intrinsic Evaluation [Lecture3] - You are required to apply Semantic-Syntactic word relationship tests for understanding of a wide variety of relationships. The example code is provided here - Word Embedding Intrinsic Evaluation (This is discussed and explained in the [Lecture5 Recording] ). You also are to visualise the result (the example can be found in the Table 2 and Figure 2 from the Original GloVe Paper) (Explain the performance)\n",
        "*** use graphs in glove paper (Lec3) with diff dim, size and models (cbow gram etc) ***\n",
        "*** code is given in colab file link or Lab02 ***\n",
        "\n",
        "**(Performance Evaluation 2 marks)**: You are to represent the precision, recall, and f1 [Lab4] of your model in the table (Explain the performance)\n",
        "\n",
        "**Hyperparameter Testing (2 marks)**: You are to provide the line graph, which shows the hyperparameter testing (with the test dataset) and explain the optimal number of epochs based on the learning rate you choose. You can have multiple graphs with different learning rates. In the graph, the x-axis would be # of epoch and the y-axis would be the f1. (Explain the performance)\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n",
        "\n",
        "## [7 Marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbLBzHObsvvM"
      },
      "source": [
        "## 3.1. Word Embedding Evaluation\n",
        "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSIUsb7qtQEf"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCrcXwcGsuuo"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr"
      },
      "source": [
        "## 3.2. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set.\n",
        "Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCF0bwTtRS0"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo"
      },
      "source": [
        "## 3.3. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.* Note that it will not be marked if you do not display it in the ipynb file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYzrA_s2tTaz"
      },
      "source": [
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f"
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hVmx4E52dXS"
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}